{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f435cde-d6d1-41af-b9cf-e46c5628510e",
   "metadata": {},
   "source": [
    "# Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).     \n",
    "- Convert text to lowercase and remove punctuation.  \n",
    "- Tokenize the text into words and sentences.  \n",
    "- Remove stopwords (using NLTK's stopwords list).\n",
    "- Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f627b694-4ce7-49b9-9fed-5058d85db82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diyak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diyak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\diyak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\diyak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.stem     import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6957eee8-6e90-4f31-ba95-16be4ae4ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food is essential for our survival and health.  \n",
      "It provides energy, nutrients, and keeps our bodies strong.  \n",
      "Different cultures have unique and delicious cuisines.  \n",
      "Fresh fruits, vegetables, and grains are very nutritious.  \n",
      "Food also brings people together during meals and celebrations.  \n",
      "Eating a balanced diet helps us live a happy, healthy life.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "new_paragraph = \"\"\"Food is essential for our survival and health.  \n",
    "It provides energy, nutrients, and keeps our bodies strong.  \n",
    "Different cultures have unique and delicious cuisines.  \n",
    "Fresh fruits, vegetables, and grains are very nutritious.  \n",
    "Food also brings people together during meals and celebrations.  \n",
    "Eating a balanced diet helps us live a happy, healthy life.\"\"\"\n",
    "new_lines = new_paragraph.split(\"\\n\")\n",
    "print(new_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60e71e9-0f64-407f-99f7-3c7142900ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['food', 'essential', 'survival', 'health', 'provides', 'energy', 'nutrients', 'keeps', 'bodies', 'strong', 'different', 'cultures', 'unique', 'delicious', 'cuisines', 'fresh', 'fruits', 'vegetables', 'grains', 'nutritious', 'food', 'also', 'brings', 'people', 'together', 'meals', 'celebrations', 'eating', 'balanced', 'diet', 'helps', 'us', 'live', 'happy', 'healthy', 'life']\n",
      "Word Frequencies:\n",
      "FreqDist({'food': 2, 'essential': 1, 'survival': 1, 'health': 1, 'provides': 1, 'energy': 1, 'nutrients': 1, 'keeps': 1, 'bodies': 1, 'strong': 1, ...})\n"
     ]
    }
   ],
   "source": [
    "# Process text: lowercase and punctuation removal\n",
    "processed_text = new_paragraph.lower()\n",
    "processed_text = processed_text.translate(str.maketrans('', '', string.punctuation))\n",
    "# .maketrans() creates a translation table mapping characters to replacements or None for deletion, used with .translate().\n",
    "\n",
    "# Break down into individual words and sentences\n",
    "individual_tokens = word_tokenize(processed_text)\n",
    "sentence_units = sent_tokenize(processed_text)\n",
    "\n",
    "# Filter out common, insignificant words\n",
    "common_words = set(stopwords.words(\"english\"))\n",
    "meaningful_words = [item for item in individual_tokens if item not in common_words]\n",
    "\n",
    "# Calculate and display word occurrences\n",
    "frequency_distribution = FreqDist(meaningful_words)\n",
    "print(\"Filtered Words:\", meaningful_words)\n",
    "print(\"Word Frequencies:\")\n",
    "frequency_distribution.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e8992-0c69-4c04-97ee-027c4aa8a46b",
   "metadata": {},
   "source": [
    "# Q2. Stemming and Lemmatization:  \n",
    "- Take the tokenized words from Question 1 (after stopword removal).  \n",
    "- Apply stemming using NLTK's PorterStemmer and LancasterStemmer.   \n",
    "- Apply lemmatization using NLTK's WordNetLemmatizer.   \n",
    "- Compare and display results of both techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3016d768-d1c8-489c-a4bd-28941dda4594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['food', 'essenti', 'surviv', 'health', 'provid', 'energi', 'nutrient', 'keep', 'bodi', 'strong', 'differ', 'cultur', 'uniqu', 'delici', 'cuisin', 'fresh', 'fruit', 'veget', 'grain', 'nutriti', 'food', 'also', 'bring', 'peopl', 'togeth', 'meal', 'celebr', 'eat', 'balanc', 'diet', 'help', 'us', 'live', 'happi', 'healthi', 'life']\n",
      "\n",
      "Lancaster Stemmer: ['food', 'ess', 'surv', 'heal', 'provid', 'energy', 'nutry', 'keep', 'body', 'strong', 'diff', 'cult', 'un', 'delicy', 'cuisin', 'fresh', 'fruit', 'veget', 'grain', 'nut', 'food', 'also', 'bring', 'peopl', 'togeth', 'meal', 'celebr', 'eat', 'bal', 'diet', 'help', 'us', 'liv', 'happy', 'healthy', 'lif']\n",
      "\n",
      "Lemmatized: ['food', 'essential', 'survival', 'health', 'provides', 'energy', 'nutrient', 'keep', 'body', 'strong', 'different', 'culture', 'unique', 'delicious', 'cuisine', 'fresh', 'fruit', 'vegetable', 'grain', 'nutritious', 'food', 'also', 'brings', 'people', 'together', 'meal', 'celebration', 'eating', 'balanced', 'diet', 'help', 'u', 'live', 'happy', 'healthy', 'life']\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "port_stemmer = PorterStemmer()\n",
    "lanc_stemmer = LancasterStemmer()\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Use filtered_words from Q1\n",
    "port_result = [port_stemmer.stem(item) for item in meaningful_words]\n",
    "lanc_result = [lanc_stemmer.stem(item) for item in meaningful_words]\n",
    "lemma_output = [word_net_lemmatizer.lemmatize(item) for item in meaningful_words]\n",
    "\n",
    "# PorterStemmer: A rule-based stemming algorithm that removes suffixes to reduce words to their root form. It follows a set of predefined rules and does not require training.\n",
    "# LancasterStemmer: A more aggressive rule-based stemming algorithm that performs faster but may be harsher in its reductions compared to the Porter Stemmer.\n",
    "# WordNetLemmatizer: Uses the WordNet lexical database to reduce words to their lemma form (dictionary form). It requires knowledge of the word's part of speech (POS) for accuracy.\n",
    "\n",
    "print(\"Porter Stemmer:\", port_result)\n",
    "print(\"\\nLancaster Stemmer:\", lanc_result)\n",
    "print(\"\\nLemmatized:\", lemma_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4a22c-c791-408a-b9fe-68bdecd80461",
   "metadata": {},
   "source": [
    "# Q3. Regular Expressions and Text Splitting:   \n",
    "- Take the original text from Question 1.   \n",
    "- Use regular expressions to:     \n",
    "    - Extract all words with more than 5 letters.  \n",
    "    - Extract all numbers (if any exist in their text).   \n",
    "    - Extract all capitalized words.    \n",
    "- Use text splitting techniques to:  \n",
    "    - Split the text into words containing only alphabets (removing digits and special characters).   \n",
    "    - Extract words starting with a vowel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a734889-641d-414c-a0a8-7f4cf1689b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words > 5 letter length: ['essential', 'survival', 'health', 'provides', 'energy', 'nutrients', 'bodies', 'strong', 'Different', 'cultures', 'unique', 'delicious', 'cuisines', 'fruits', 'vegetables', 'grains', 'nutritious', 'brings', 'people', 'together', 'during', 'celebrations', 'Eating', 'balanced', 'healthy']\n",
      "\n",
      "Numbers: []\n",
      "\n",
      "Capitalized words: ['Food', 'It', 'Different', 'Fresh', 'Food', 'Eating']\n",
      "\n",
      "Alphabet-only words: ['Food', 'is', 'essential', 'for', 'our', 'survival', 'and', 'health', 'It', 'provides', 'energy', 'nutrients', 'and', 'keeps', 'our', 'bodies', 'strong', 'Different', 'cultures', 'have', 'unique', 'and', 'delicious', 'cuisines', 'Fresh', 'fruits', 'vegetables', 'and', 'grains', 'are', 'very', 'nutritious', 'Food', 'also', 'brings', 'people', 'together', 'during', 'meals', 'and', 'celebrations', 'Eating', 'a', 'balanced', 'diet', 'helps', 'us', 'live', 'a', 'happy', 'healthy', 'life']\n",
      "\n",
      "Words starting with vowels: ['is', 'essential', 'our', 'and', 'It', 'energy', 'and', 'our', 'unique', 'and', 'and', 'are', 'also', 'and', 'Eating', 'a', 'us', 'a']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input string for analysis\n",
    "input_string = new_paragraph\n",
    "\n",
    "# Find words exceeding five characters\n",
    "long_strings = re.findall(r'\\b\\w{6,}\\b', input_string)\n",
    "\n",
    "# Extract any sequence of digits\n",
    "found_numbers = re.findall(r'\\d+', input_string)\n",
    "\n",
    "# Identify words starting with a capital letter followed by lowercase letters\n",
    "leading_capital_words = re.findall(r'\\b[A-Z][a-z]*\\b', input_string)\n",
    "\n",
    "# Select words containing only alphabetic characters\n",
    "pure_alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', input_string)\n",
    "\n",
    "# Filter words that commence with a vowel\n",
    "starting_vowel_words = [item for item in pure_alpha_words if item[0].lower() in 'aeiou']\n",
    "\n",
    "print(\"Words > 5 letter length:\", long_strings)\n",
    "print(\"\\nNumbers:\", found_numbers)\n",
    "print(\"\\nCapitalized words:\", leading_capital_words)\n",
    "print(\"\\nAlphabet-only words:\", pure_alpha_words)\n",
    "print(\"\\nWords starting with vowels:\", starting_vowel_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d165d-c759-453e-a8c6-8ca723496e46",
   "metadata": {},
   "source": [
    "# Q4. Custom Tokenization & Regex-based Text Cleaning:   \n",
    "- Take original text from Question 1   \n",
    "- Write a custom tokenization function that:   \n",
    "    - Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\").   \n",
    "    - Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).   \n",
    "    - Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as it is).   \n",
    "- Use Regex Substitutions (re.sub) to:   \n",
    "    - Replace email addresses with `<EMAIL>` placeholder.   \n",
    "    - Replace URLs with `<URL>` placeholder.   \n",
    "    - Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with `<PHONE>` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0028bce9-92a4-4832-8374-742e520d792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokens: ['Contact', 'us', 'at', 'support', 'website', 'com', 'See', 'more', 'at', 'https', 'website', 'info', 'or', 'ring', 'us', 'at', '91', '7777777777']\n",
      "Cleaned Text: Contact us at <EMAIL> See more at <URL> or ring us at <PHONE>.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example string containing contact information\n",
    "example_string = \"Contact us at support@website.com. See more at https://website.info or ring us at +91 7777777777.\"\n",
    "\n",
    "# Specialized token extraction method\n",
    "def specialized_token_extraction(text):\n",
    "    # Preserve hyphenated and apostrophed words, as well as decimal and whole numbers\n",
    "    return re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b|\\d+\\.\\d+|\\d+\", text)\n",
    "\n",
    "# Generate tokens from the example string\n",
    "generated_tokens = specialized_token_extraction(example_string)\n",
    "\n",
    "# Replace identifiable patterns with placeholders\n",
    "modified_text = re.sub(r'\\S+@\\S+', '<EMAIL>', example_string)\n",
    "modified_text = re.sub(r'https?://\\S+', '<URL>', modified_text)\n",
    "modified_text = re.sub(r'\\+91\\s?\\d{10}|\\d{3}-\\d{3}-\\d{4}', '<PHONE>', modified_text)\n",
    "\n",
    "print(\"Custom Tokens:\", generated_tokens)\n",
    "print(\"Cleaned Text:\", modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c7602-3b33-4eae-851a-1667db4f5511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11599c3-8a6d-407c-bdc6-0fe8cd7baebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13bd83f-3122-4032-9269-a393730aedd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea803ff1-9bb3-474e-adb5-b1d0dc0606ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353d6a5-6451-4e00-bd1b-ed074cfeff00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
